# Conceptual Understanding Notes

Perfect. What youâ€™ve written is a brilliant cross-section of the **modern AI engineerâ€™s mind** â€” one foot in statistics, the other in self-attention. Letâ€™s reformat this into clean, scannable **Markdown flashcards**, neatly separated into **GenAI / LLMOps** and **Classical Data Science** sections.

Each â€œflashcardâ€ follows the format:
**Q:** (Question)
**A:** (Answer + concise insight ğŸ’¡)

<object data="interviews/mlops.html" type="text/html"></object>
---

# ğŸ§  AI Interview Flashcards

**Theme:** Bridging *Classical Data Science* and *Modern GenAI Engineering*
*(Use these for quick pre-interview refreshers or team brain workouts.)*

---

## âš¡ Part I: Modern GenAI & LLMOps

### ğŸ§© 1. Chunking Mechanisms in RAG

**Q:** What are the different chunking mechanisms used in RAG, and why do they matter?
**A:**
Chunking splits large documents into meaningful pieces for embedding and retrieval.

* **Fixed-size:** Simple token limits (e.g., 500 tokens).
* **Semantic:** Split by sentences or topic boundaries.
* **Recursive:** Hierarchical, meaning-based chunking.
* **Overlapping:** Add small overlaps to preserve continuity.
  ğŸ’¡ *Better chunking = fewer hallucinations + higher retrieval accuracy.*

---

### âš™ï¸ 2. Transformer Architecture

**Q:** Whatâ€™s the key idea behind the Transformer architecture?
**A:**
Transformers replaced RNNs with **self-attention**, letting each token attend to every other token in parallel.
Core blocks: *embeddings â†’ positional encodings â†’ multi-head attention â†’ feedforward â†’ residuals â†’ normalization.*
ğŸ“˜ **Encoder (BERT)** understands. **Decoder (GPT)** generates. **T5** does both.

---

### ğŸ”§ 3. LLM Fine-Tuning Mechanisms

**Q:** How do you fine-tune large language models efficiently?
**A:**
Use **Parameter-Efficient Fine-Tuning (PEFT)**:

* **LoRA:** Add low-rank adapters.
* **Prefix / Prompt Tuning:** Train task-specific prefixes.
* **Adapters:** Tiny plug-ins in layers.
* **Distillation:** Teach a smaller model.
  âš¡ *Freeze most weights â€” train only what matters.*

---

### ğŸš€ 4. Optimizing LLM Inference

**Q:** How can we make LLM inference faster and cheaper?
**A:**

* **Quantization:** FP32 â†’ INT8
* **Batching:** Handle multiple queries per GPU
* **KV Caching:** Reuse attention states
* **Speculative Decoding:** Draft + verify approach
* **Parallelism:** Distribute across GPUs
* **Distillation:** Deploy smaller models
  ğŸ§  *Think: cache, quantize, parallelize.*

---

### ğŸª„ 5. Prompting Techniques

**Q:** What are key prompting techniques in GenAI?
**A:**

* **Zero-shot:** Just the question.
* **Few-shot:** Add examples.
* **Chain-of-Thought:** Step-by-step reasoning.
* **Self-Consistency:** Multiple reasoning paths â†’ majority answer.
* **ReAct:** Reason + act using tools.
  ğŸ§© *Prompts are the new programming language â€” version them like code.*

---

### ğŸ§± 6. Agentic AI Frameworks

**Q:** What are Agentic AI frameworks?
**A:**
Frameworks like **CrewAI**, **AutoGen**, and **TaskWeaver** orchestrate multiple specialized LLM agents that collaborate, use tools, and maintain memory to complete multi-step tasks.
ğŸ’¡ *Shift from reactive chatbots â†’ proactive, reasoning systems.*

---

### ğŸ§© 7. RAG Fundamentals

**Q1:** What is Retrieval-Augmented Generation (RAG)?
**A:**
RAG connects an LLM to an external knowledge base via embeddings and vector search.
ğŸ’¡ *Preferred over fine-tuning for fast, cheap, and updatable domain knowledge.*

**Q2:** Why is RAG preferred over fine-tuning for enterprises?
**A:**
Avoids retraining, allows real-time knowledge updates, reduces hallucination, improves traceability.

**Q3:** Outline RAG data flow.
**A:**
User query â†’ Query embedding â†’ Vector search â†’ Retrieve top chunks â†’ Augment prompt â†’ LLM generates grounded response.
ğŸ’¡ *Think: â€œRetrieve, then Read.â€*

**Q4:** Key components of RAG system?
**A:**
Embedding model, vector store, retriever, prompt composer, LLM, evaluation/logging.

**Q5:** Limitations of RAG?
**A:**
Retrieval errors, poor chunking, latency, embedding drift, limited context window.
ğŸ’¡ *Mitigate with hybrid search, semantic chunking, and RAGAS.*

---

### ğŸ§© 8. Prompt Optimization

**Q:** How do you optimize prompts for accuracy and cost?
**A:**
Clear, structured, parameterized templates.
Track versions (LangFuse).
Compress context, use smaller models for subtasks.
ğŸ’¡ *Prompt engineering = versioning + evaluation + clarity.*

---

### ğŸ§­ 9. LLM Observability

**Q:** What is LLM observability and why is it important?
**A:**
Tracks LLM performance, latency, and cost.
Metrics: recall@k, latency, token usage, feedback, faithfulness.
Tools: LangFuse, LangTrace, Prometheus, Grafana.
ğŸ’¡ *Observability = reliability + continuous learning.*

---

### ğŸ” 10. GenAI Evaluation

**Q:** How do you evaluate GenAI model responses?
**A:**

* **Offline:** RAGAS, TruLens, Giskard, human review.
* **Online:** Log queries in LangFuse/LangTrace; use LLM-as-a-judge.
  ğŸ’¡ *Evaluation closes the feedback loop.*

---

### ğŸŒ 11. Scaling and Deployment

**Q:** How do you deploy RAG/Agentic systems at scale?
**A:**
Kubernetes auto-scaling, caching (Redis), scalable vector DBs (Weaviate/Milvus), async via Kafka, distributed state stores.
ğŸ’¡ *Goal: resilient, observable, cost-aware infra.*

---

### âš™ï¸ 12. GenAI Cost & Performance Optimization

**Q:** How do you optimize GenAI systems?
**A:**
Response caching, batching, token optimization, hybrid retrieval, distillation.

---

### ğŸ§­ 13. Model Drift

**Q:** What is model drift in RAG or agent pipelines?
**A:**
When embeddings, prompts, or retrieval degrade due to updates.
ğŸ’¡ *Mitigate via re-embedding, dashboards, version control.*

---

### ğŸ§© 14. LLMOps Tooling

**Q:** Which tools are common in LLMOps?
**A:**

* **LangChain / LangGraph** â€” orchestration
* **LangFuse / LangTrace** â€” observability
* **RAGAS / TruLens** â€” evaluation
* **MLflow** â€” tracking
* **Kubernetes / Docker** â€” deployment
* **CrewAI / AutoGen** â€” agent orchestration

---

### ğŸ›¡ï¸ 15. Fault Tolerance in GenAI Systems

**Q:** How do you ensure fault tolerance?
**A:**
Multi-region deployment, circuit breakers, fallback models, persistent caching.
ğŸ’¡ *Resilience isnâ€™t a luxury â€” itâ€™s design hygiene.*

---

### ğŸ§© 16. LLM Text Generation Parameters

**Q:** What are the key text generation parameters in large language models (LLMs), and how do they affect output?

**A:**
These parameters control the *style*, *diversity*, and *determinism* of model responses.

* **`temperature`** â€“ Controls randomness.

  * Low (â‰ˆ0.1â€“0.3): Deterministic, factual answers.
  * High (â‰ˆ0.8â€“1.0): Creative, varied outputs.
    ğŸ’¡ *Think of it as the modelâ€™s â€œspice level.â€*

* **`top_k`** â€“ Keeps only the top *k* most probable next tokens.

  * Small *k* (e.g., 20): Safer, focused text.
  * Large *k* (e.g., 100): More variety.
    ğŸ’¡ *Prunes the long tail of unlikely words.*

* **`top_p` (nucleus sampling)** â€“ Chooses from smallest token set whose probabilities sum to *p*.

  * Common range: 0.8â€“0.95.
    ğŸ’¡ *Balances diversity and coherence more smoothly than top_k.*

* **`max_tokens`** â€“ Caps output length.

  * Prevents runaway responses and controls cost.
    ğŸ’¡ *Useful for APIs with per-token pricing.*

* **`repetition_penalty` / `presence_penalty` / `frequency_penalty`** â€“ Reduce repeated phrases or overused words.
  ğŸ’¡ *Encourages linguistic variety and discourages loops.*

* **`stop` tokens** â€“ Define where generation should end.
  ğŸ’¡ *Useful for structured formats like JSON or conversations.*

* **`beam_search`** (less common in chat models) â€“ Explores multiple likely continuations before choosing the best.
  ğŸ’¡ *Used for tasks needing precision over creativity.*

---

ğŸ’¡ *In short:*
`temperature` = creativity,
`top_p/top_k` = diversity filter,
`penalties` = repetition control,
`max_tokens` = budget guardrail.

ğŸ§  *Mastering these dials turns you from a prompt engineer into a dialogue composer.*

---

## ğŸ“Š Part II: Classical Data Science Concepts

### ğŸ§  1. Activation Functions

**Q:** Why are activation functions crucial in deep learning?
**A:**
They add non-linearity, letting networks learn complex relationships.

* **ReLU:** Fast, avoids vanishing gradients.
* **Leaky ReLU:** Fixes â€œdeadâ€ neurons.
* **Sigmoid/Tanh:** Older, saturate quickly.
* **GELU:** Probabilistic ReLU used in Transformers.
  ğŸ§© *Modern LLMs = LayerNorm + GELU + residuals + attention magic.*

---

### âš™ï¸ 2. Regularization Techniques

**Q:** How do we keep models from overfitting?
**A:**
L1 (Lasso), L2 (Ridge), Dropout, Early stopping, Weight decay, Data augmentation.
ğŸ§  *Regularization = teaching the model humility.*

---

### ğŸ“Š 3. Classical ML Core Concepts

**Q:** What classical ML ideas still matter today?
**A:**
Regression, classification, clustering, tree-based models (XGBoost, RF), feature scaling, AUC/F1/MSE, cross-validation, bias-variance tradeoff.
ğŸ“Š *LLMs love text; XGBoost still rules the tables.*

---

# ğŸš€ Closing Thought

The best AI engineers are *bilingual*: fluent in **statistical reasoning** and **systems design**.
If you can move from **sigmoid to self-attention** with conceptual clarity, youâ€™re already ahead of 90% of the field.

Keep these flashcards handy â€” your neurons deserve a good warm-up before the next interview.
